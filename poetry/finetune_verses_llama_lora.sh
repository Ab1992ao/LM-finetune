python -m torch.distributed.run --nproc_per_node=2 finetune_llama_lora.py \
--dataset_path ~/polygon/text_generator/tmp/лирика.jsonl \
--max_samples 10000 \
--output_syllables 0 \
--model_name_or_path decapoda-research/llama-7b-hf \
--output_dir ~/polygon/text_generator/tmp/verses_model=llama7b_lora_domain=lyrics_syllables=0 \
--overwrite_output_dir 1 \
--per_device_train_batch_size 1 \
--learning_rate 1e-4 \
--num_train_epochs 1 \
--bf16 0 \
--fp16 0 \
--gradient_checkpointing 0 \
--gradient_accumulation_step 8 \
--do_train 1 \
--do_eval 0 \
--report_to tensorboard \
--evaluation_strategy no \
--logging_strategy steps \
--logging_steps 200 \
--save_strategy no \
