python -m torch.distributed.launch --nproc_per_node=2 finetune_llama.py \
--dataset_path ~/polygon/text_generator/tmp/лирика.jsonl \
--max_samples 10000 \
--output_syllables 0 \
--model_name_or_path decapoda-research/llama-7b-hf \
--output_dir ~/polygon/text_generator/tmp/verses_model=llama7b_domain=lyrics_syllables=0 \
--overwrite_output_dir 1 \
--per_device_train_batch_size 8 \
--learning_rate 1e-5 \
--num_train_epochs 1 \
--bf16 1 \
--fp16 0 \
--gradient_checkpointing 0 \
--gradient_accumulation_step 8 \
--do_train 1 \
--do_eval 0 \
--report_to tensorboard \
--evaluation_strategy no \
--logging_strategy steps \
--logging_steps 10 \
--save_strategy no \
--deepspeed /home/jovyan/13b_pretrain_119000/model/13b_deepspeed_config.json
